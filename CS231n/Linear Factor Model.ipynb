{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev1 toc-item\"><a href=\"#Probabilistic-PCA-and-Factor-Analysis\" data-toc-modified-id=\"Probabilistic-PCA-and-Factor-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Probabilistic PCA and Factor Analysis</a></div><div class=\"lev1 toc-item\"><a href=\"#Independent-Component-Analysis\" data-toc-modified-id=\"Independent-Component-Analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Independent Component Analysis</a></div><div class=\"lev1 toc-item\"><a href=\"#Slow-Feature-Analysis\" data-toc-modified-id=\"Slow-Feature-Analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Slow Feature Analysis</a></div><div class=\"lev1 toc-item\"><a href=\"#Sparse-Coding\" data-toc-modified-id=\"Sparse-Coding-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Sparse Coding</a></div><div class=\"lev1 toc-item\"><a href=\"#Manifold-Interpretation-of-PCA\" data-toc-modified-id=\"Manifold-Interpretation-of-PCA-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Manifold Interpretation of PCA</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A linear factor model is defined by the use of a stochastic, linear decoder function that** *generates $x$ by adding noise to a linear transformation of $h$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> latent variables $h$, with $p_{model}(x)=E_{h}p_{model}(x|h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear factor model describes the data generation process as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sampling the explanatory factors $h$ from a distribution $h \\sim p(h)$, where $p(h)$ is a factorial distribution, with $p(h)=\\prod_{i}p(h_{i})$\n",
    "2. Sampling the real-valued observable variables given the factors: $x=Wh+b+noise$, where the noise is typically Gaussian and diagonal (independent across dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic PCA and Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special cases of the above equations and only differ in the choices made for the model's prior over **latent variables $h$** before observing $x$ and noise distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **latent variable prior** is the unit variance Gaussion $h \\sim N(h;0,I)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed variables $x_i$ are asssumed to be *conditionally independent*, given $h$, the noise is assumed to be drawn from a diagonal covariance Gaussian distribution $\\psi = diag(\\sigma^2)$, with $\\sigma^2 = [\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_n^2]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The role of the latent variables is to *capture the dependencies* between the different observed variables $x_i$, $x$ is indeed a multivariate normal random variables:\n",
    "$$x \\sim N(x:, b, WW^T+\\psi)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification: making the conditional variances $\\sigma_i^2$ equal to each other.\n",
    "$$ x \\sim N(x:,b,WW^T+\\sigma^2 I) $$\n",
    "or equivalently\n",
    "$$ x=Wh+b+\\sigma z $$, where $$z \\sim N(z;0,I) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, using an iterative EM algorithm for estimating the parameters $W$ and $\\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: most variations in the data can be captured by the latent variables $h$, up to some (except) small residual *reconstruction error $\\sigma^2$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The concept of probabilistic PCA model** : when $\\sigma \\rightarrow 0$, the conditional expected value of $h$ given $x$ becoms an orthogonal projection of $x-b$ onto the space spanned by the $d$ columns of $W$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ICA** is an approach to modeling linear factors that seeks to separate an observed signal into many underlying signals that are scaled and added together to form the observed data.\n",
    "**These signals are intended to be fully independent, rather than merely decorrelated from each other**\n",
    "> Uncorrelated variables is not equal to independent, uncorrelation means the covariances are 0, but the independency means that the variables are probabilistic multiplicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variant ICA trains a fully parametric generative model. The prior distribution over the underlying factors, $p(h)$ must be fixed ahead of time by the user. Then **deterministically** generates $x=Wh$\n",
    "\n",
    "> A nonlinear change of variables to determine $p(x)$, like $$p_x(x)=p_y(g(x))|det(\\frac{\\partial g(x)}{\\partial x})|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By choosing $p(h)$ to be independent, we can recover underlying factors that are as close as possible to independent.\n",
    "> not to capture high-level abstract causal factors, but to recover low-level signals that have been mixed together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Variants of ICA\n",
    "    > - Add some noise in the generation of $x$\n",
    "    > - Do not use the maximum likelihood criterion, but instead aim to make the elemtns of $h=W^{-1}x$ independent from each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** all variants of ICA require that $p(h)$ be non-Gaussion\n",
    "> Why: If $p(h)$ is Gaussion distribution, the solution of $W$ is not unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** Many variants of ICA are not generative models in the sense that we use the phrase. **A generative model either represents $p(x)$ or can draw samples from it*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Extensions of ICA: nonlinear independent components estimation (NICE) (2014)*\n",
    "- *Extensions of ICA: learn groups of features, with statistical dependence allowed within a group but discouraged between groups*\n",
    "> - *independent subspace analysis*, the groups of related units are chosen to be non-overlapping\n",
    "> - *topographic ICA*, assign spatially coordinates to each hidden unit and form overlapping groups of spatially neighboring units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slow Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SFA is a linear facor model that uses information from time signals to learn invariant features (2002)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivation: slowness principle. *Compared with the individual measurements that make up a description of a scene*, the important characteristics of scenes change very slowly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slowness principle may be introduced by adding a term to the cost function of the form $$\\lambda \\sum_t L(f(x^{(t+1)}), f(x^{(t)}))$$ where $\\lambda$ is a hyperparameter determining the strength of the slowness regularization term, $f$ is the feature extractor to be regularized, and $L$ is a loss function measuring the distance, a common choice is the mean squared difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possibly to theoretically predict which features SFA will learn, even in the deep, nonlinear setting. To make such theoretical predictions, one must know about the dynamics of the environment in terms of configuration space. Given the knowledge of how the underlying facors actually change, it is possible to analytically solve for the optimal fuctions expressing these factors.\n",
    "\n",
    "Other learning algorithms where the cost function depends highly on specific pixel values, making it much more difficult to determin what features the model will learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">P. Berkes and L. Wiskott. Slow feature analysis yields arich repertoire of complex cell properties. Journal of Vision,2005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sparse coding** uses a linear decoder plus noise to obtain reconstructions of $x$, $$p(x|h)=N(x;Wh+b,\\frac{1}{\\beta}I)$$. Assuming that the linear factors have Gaussian noise with isotropic precision $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution $p(h)$ is chosen to be one with sharp peaks near 0, including factorized Laplace, Cauchy or factorized Student-t distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Examples:*\n",
    "\n",
    "- Laplace prior with the sparsity penalty coefficient $\\lambda$ is given by $$p(h_i)=Laplace(h_i;0, \\frac{2}{\\lambda})=\\frac {\\lambda}{4} e^{-\\frac{1}{2} \\lambda |h_i|}$$\n",
    "- Student-t prior by $$p(h_i) \\propto \\frac{1}{(1+\\frac{h_i^2}{v})^{\\frac{\\lambda+1}{2}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder:** $$h^*=f(x)=\\arg\\max_h p(h|x)\\\\=\\arg\\max_h log p(h|x)\\\\=\\arg\\min_h \\lambda ||h||_1 + \\beta ||x-Wh||_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We alternate between minimization with respect to $h$ and minimization with respect to $W$. We treat $\\beta$ as a hyper, typically is set to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The generative model itself is not especially sparse, only the feature extractor is.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse coding approach combined with the use of the **non-parametric encoder** can in principle minimize the combination of reconstruction error and log-prior better than any specific **parametric encoder**. Besides, there is no generalization error to the **encoder**.\n",
    "\n",
    "**Disadvantage**: the non-parametric encoder requires running an iterative algorithm while parametric autoencoder approach uses only a fixed number of layers; it is not straight-forward to back-propagate through the non-parametric encoder -- difficult to pretrain a sparse coding model with an unsupervised criterion and then fine-tune it using a supervised criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the model is able to reconstruct the data well and provide useful features for a classifier, the samples produced by sparse coding may still be poor.\n",
    "\n",
    "- Each individual feature may be learned well, but the factorial prior on the hidden code makes the model including random subsets of all of the features in each generated sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Interpretation of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear factor models including PCA and factor analysis can be interpreted as learning a manifold (Hinton97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notes: Flat Gaussiian capturing probability concentration near a low-dimensional manifold*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the encoder be $h=f(x)=W^T(x-\\mu)$\n",
    "\n",
    "With the autoencoder view, the decoder computing the reconstruction $\\widehat{x}=g(h)=b+Vh$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task of PCA that learns matrices $W$ and $V$ with the goal of making the reconstruction of $x$ lie as close to $x$ as possible.\n",
    "\n",
    "* The choices of linear encoder and decoder minimize reconstruction error $E[||x-\\widehat{x}||^2]$\n",
    "* Correspond to $V=W$, $\\mu=b=E[x]$ and the columns of $W$ form an orthonormal basis which spans the same subspace as the principal eigenvectors of the covariance matrix $C=E[(x-\\mu)(x-\\mu)^T]$\n",
    "* the eigenvalue $\\lambda_i$ of $C$ corresponds to the variance of $x$ in the direction of eigenvector $v^{(i)}$. If $x \\in R^D$ and $h \\in R^d$ with $d < D$, then the oprimal reconstruction error is $\\min E[||x-\\widehat{x}||^2]=\\sum_{i=d+1}^D \\lambda_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear factor models are some of the simplest generative models and some of the simplest models that learn a representation of data**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "122px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
