{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#基础\" data-toc-modified-id=\"基础-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>基础</a></div><div class=\"lev2 toc-item\"><a href=\"#基础知识\" data-toc-modified-id=\"基础知识-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>基础知识</a></div><div class=\"lev2 toc-item\"><a href=\"#结构\" data-toc-modified-id=\"结构-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>结构</a></div><div class=\"lev3 toc-item\"><a href=\"#边\" data-toc-modified-id=\"边-121\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>边</a></div><div class=\"lev3 toc-item\"><a href=\"#节点\" data-toc-modified-id=\"节点-122\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>节点</a></div><div class=\"lev3 toc-item\"><a href=\"#其他\" data-toc-modified-id=\"其他-123\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>其他</a></div><div class=\"lev3 toc-item\"><a href=\"#内核\" data-toc-modified-id=\"内核-124\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>内核</a></div><div class=\"lev2 toc-item\"><a href=\"#常用API\" data-toc-modified-id=\"常用API-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>常用API</a></div><div class=\"lev3 toc-item\"><a href=\"#图，操作和张量\" data-toc-modified-id=\"图，操作和张量-131\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>图，操作和张量</a></div><div class=\"lev3 toc-item\"><a href=\"#可视化\" data-toc-modified-id=\"可视化-132\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>可视化</a></div><div class=\"lev2 toc-item\"><a href=\"#变量作用域\" data-toc-modified-id=\"变量作用域-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>变量作用域</a></div><div class=\"lev3 toc-item\"><a href=\"#variable_scope\" data-toc-modified-id=\"variable_scope-141\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>variable_scope</a></div><div class=\"lev3 toc-item\"><a href=\"#name_scope\" data-toc-modified-id=\"name_scope-142\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>name_scope</a></div><div class=\"lev2 toc-item\"><a href=\"#批标准化\" data-toc-modified-id=\"批标准化-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>批标准化</a></div><div class=\"lev2 toc-item\"><a href=\"#神经元函数及优化方法\" data-toc-modified-id=\"神经元函数及优化方法-16\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>神经元函数及优化方法</a></div><div class=\"lev3 toc-item\"><a href=\"#激活函数\" data-toc-modified-id=\"激活函数-161\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>激活函数</a></div><div class=\"lev3 toc-item\"><a href=\"#卷积函数\" data-toc-modified-id=\"卷积函数-162\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>卷积函数</a></div><div class=\"lev3 toc-item\"><a href=\"#池化函数\" data-toc-modified-id=\"池化函数-163\"><span class=\"toc-item-num\">1.6.3&nbsp;&nbsp;</span>池化函数</a></div><div class=\"lev3 toc-item\"><a href=\"#分类函数\" data-toc-modified-id=\"分类函数-164\"><span class=\"toc-item-num\">1.6.4&nbsp;&nbsp;</span>分类函数</a></div><div class=\"lev3 toc-item\"><a href=\"#优化方法\" data-toc-modified-id=\"优化方法-165\"><span class=\"toc-item-num\">1.6.5&nbsp;&nbsp;</span>优化方法</a></div><div class=\"lev2 toc-item\"><a href=\"#模型的存储与加载\" data-toc-modified-id=\"模型的存储与加载-17\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>模型的存储与加载</a></div><div class=\"lev3 toc-item\"><a href=\"#模型的存储与加载\" data-toc-modified-id=\"模型的存储与加载-171\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>模型的存储与加载</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 基础知识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add_1:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "t = tf.add(8, 9)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tensorflow中涉及的运算都在图中，而图的运行只发生在session中。开启session后，就可以用数据填充node，进行运算；关闭session后就不能进行计算了。因此，**会话提供了操作运行和Tensor求值的环境**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.  8.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1.0, 2.0])\n",
    "b = tf.constant([3.0, 4.0])\n",
    "c = a * b\n",
    "\n",
    "#Create a session\n",
    "sess = tf.Session()\n",
    "\n",
    "#Compute the tensor c\n",
    "print(sess.run(c))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 边"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tensorflow的边有两种连接关系：数据依赖和控制依赖。实线边表示数据依赖，代表数据，即张量。虚线边表示控制依赖，用于控制操作的运行，这类边上没有数据流过，但源节点必须在目的节点开始执行前完成执行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "节点又称为算子（OP），一般用来表示施加的数学运算，也可以表示数据输入（feed in）的起点以及输出（push out）的终点；或是读取/写入持久变量（persistent variable）的终点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "|类别         |示例             |\n",
    "|-----------|--------------|\n",
    "|数学运算  |Add,Sub,Mul,Div,Exp,Log,Greater,Less,Equal...|\n",
    "|数组运算  |Concat,Slice,Split,Constant,Rank,Shape,Shuffle...|\n",
    "|矩阵运算  |MatMul,MatrixInverse,MatrixDeterminant...|\n",
    "|有状态的操作|Variable，Assign，AssignAdd...|\n",
    "|神经网络构建操作|SoftMax,Sigmoid, ReLU,Convolution2D,MaxPool|\n",
    "|检查点操作|Save,Restore|\n",
    "|队列和同步操作|Enqueue,Dequeue,MutexAcquire,MutexRelease...|\n",
    "|控制张量流动的操作|Merge,Switch,Enter,Leave,NextIteration...|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 其他"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* 图：操作任务可以描述成有向无环图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "matrix1 = tf.constant([[3., 3.]])  #边\n",
    "matrix2 = tf.constant([[2.],[2.]]) #边\n",
    "\n",
    "product = tf.matmul(matrix1, matrix2) #节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* 会话：创建一个会话来启动图，会话提供在图中执行操作的一些方法。一般模式是建立会话（此时是空图），在会话中添加节点和边，形成一张图，然后执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 12.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session() #空图\n",
    "result = sess.run([product])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> 会话主要有两个API接口，Extend（在图中添加节点和边），Run（输入计算的节点和填充必要的数据后进行运算，并输出运算结果）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* 设备：Tensorflow可以指定在哪个设备上执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#with tf.Session() as sess:\n",
    "#    with tf.device(\"/gpu:0\"): #指定在第一个gpu上执行\n",
    "#        matrix1 = tf.constant([[3., 3.]]) \n",
    "#        matrix2 = tf.constant([[2.],[2.]])\n",
    "#        product = tf.matmul(matrix1, matrix2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* 变量：在图中有固定的位置，不能像张量（边）一样随意流动。创建变量张量，使用```tf.Variable()```构造函数，需要初始值。初始值包括形状（shape）和类型（type）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'counter_16:0' shape=() dtype=int32_ref>\n"
     ]
    }
   ],
   "source": [
    "#标量\n",
    "state = tf.Variable(0, name=\"counter\")\n",
    "\n",
    "print(state) #注意这里没有体现出state的实值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "#常量张量\n",
    "input1 = tf.Variable(3.0)\n",
    "\n",
    "print(input1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tensorflow提供填充机制，可以在构建图时使用```tf.placeholder()```临时替代任意操作的张量，在调用Session对象的```run()```方法去执行图时，使用填充数据作为调用的参数，调用结束后，填充数据就消失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 21.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "output = tf.multiply(input1, input2)\n",
    "\n",
    "with tf.Session() as sess1:       \n",
    "    print(sess1.run([output], feed_dict={input1:[7.], input2:[3.]}))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 内核"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "内核（kernel）时能够运行在特定设备（如CPU，GPU）上的一种对操作的实现。同一个操作可能会对应多个内核。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 常用API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 图，操作和张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "表示计算的操作对象```tf.Graph```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "|操作|描述|\n",
    "|----|-----|\n",
    "|tf.Graph.\\_\\_init\\_\\_()|创建一个空图|\n",
    "|tf.Graph.as_default|将某图设为默认图。如果不显示设置，系统会自动设置一个全拒的默认图。在模块范围内定义的节点都默认加入默认图中|\n",
    "|tf.Graph.device(device_name_or_function)|定义运行图所使用的设备|\n",
    "|tf.Graph.name_scope(name)|为节点创建层次化的名称|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "tf.Operation类表示图中的一个节点，用于计算张量数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "|操作|描述|\n",
    "|----|----|\n",
    "|tf.Operation.name|操作的名称|\n",
    "|tf.Operation.type|操作的类型|\n",
    "|tf.Operation.inputs/output|操作的输入与输出|\n",
    "|tf.Operation.control_inputs|操作的依赖|\n",
    "|tf.Operation.run(feed_dict=None,session=None)|在会话中运行该操作|\n",
    "|tf.Operation.get_attr(name)|获得操作的属性|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "tf.Tensor类时操作输出的符号句柄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "|操作|描述|\n",
    "|---|--|\n",
    "|tf.Tensor.dtype|张量的数据类型|\n",
    "|tf.Tensor.name|名称|\n",
    "|tf.Tensor.value_index|张量在操作输出中的索引|\n",
    "|tf.Tensor.graph|张量所在的图|\n",
    "|tf.Tensor.op|产生该张量的操作|\n",
    "|tf.Tensor.consumers()|返回使用该张量的操作列表|\n",
    "|tf.Tensor.eval(feed_dict=None, session=None)|在会话中求张量的值|\n",
    "|tf.Tensor.get_shape()|返回用于表示张量的形状的类TensorShape|\n",
    "|tf.Tensor.set_shape(shape)|更新张量的形状|\n",
    "|tf.Tensor.device|设置计算该张量的设备|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "在程序中给必要**节点**添加摘要（summary）。摘要会收集数据，写入事件文件（event file）。tf.summary.FileWriter类用于在目录中创建事件文件，并添加摘要和事件，用来在TensorBoard中展示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 变量作用域"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### variable_scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "variable_scope主要给variable_name加前缀。\n",
    "\n",
    "```v=tf.get_variable(name, shape, dtype, initializer) #通过名字创建或者返回一个变量\n",
    "tf.variable_scope(<scope_name>) #为变量指定命名空间\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* 获取变量作用域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "with tf.variable_scope(\"foo\") as foo_scope:\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "with tf.variable_scope(foo_scope):\n",
    "    w = tf.get_variable(\"w\", [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* 变量作用域初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "variable_scope主要用于RNN共享变量的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### name_scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "name_scope会影响op_name，不会影响用get_variable创建的变量，但是会印象用过Variable()创建的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fooo/v:0\n",
      "fooo/bar/b:0\n",
      "fooo/bar/add:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.variable_scope(\"fooo\"):\n",
    "    with tf.name_scope(\"bar\"):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "        b = tf.Variable(tf.zeros([1]), name = 'b')\n",
    "        x = 1.0 + v\n",
    "#assert v.name == \"fooo/v:0\"\n",
    "#assert b.name == \"fooo/bar/b:0\"\n",
    "#assert x.name == \"fooo/bar/add\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(v.name)\n",
    "    print(b.name)\n",
    "    print(x.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* 创建一个新的variable_scope时不需要把reuse属性设置未False，只需要在使用的时候设置为True就可以了。\n",
    "* TensorFlow里创建变量的两种方式有 tf.get_variable() 和 tf.Variable() ,在 tf.name_scope下时，tf.get_variable()创建的变量名不受 name_scope 的影响，而且在未指定共享变量时，如果重名会报错，tf.Variable()会自动检测有没有变量重名，如果有则会自行处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 批标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "引入批标准化来规范化某些层或者所有层的输入，从而固定每层输入信号的均值和方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "批标准化一般用在非线性映射（激活函数）之前，对$x=Wh+b$做规范化，使结果（输出信号各个维度）的均值为0，方差为1。让每一层的输入有一个稳定的分布会有利于网络的训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* 批标准化的优点是通过规范化使得激活函数分布在线性区间，从而加大了梯度，让模型更大胆地进行梯度下降\n",
    ">* 加大探索的步长，加快收敛\n",
    ">* 更容易跳出局部最小值\n",
    ">* 破坏原来的数据分布，一定程度上缓解过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "import tensorflow as tf\n",
    "fc_mean, fc_var = tf.nn.moments(Wx_plus_b, axes=[0],)\n",
    "#fc_mean,fc_var是Wx_plus_b的均值和方差\n",
    "scale = tf.Variable(tf.ones([out_size]))\n",
    "shift = tf.Variable(tf.zeros([out_size]))\n",
    "epsilon = 0.001\n",
    "Wx_plus_b = tf.nn.batch_normalization(Wx_plus_b, fc_mean, fc_var, shift, scale, epsilon)\n",
    "#Wx_plus_b = (Wx_plus_b - fc_mean) / tf.sqrt(fc_var + 0.001)\n",
    "#Wx_plus_b = Wx_plus_b * scale + shift\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经元函数及优化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活函数不会更改输入数据的维度，包括平滑非线性的激活函数sigmoid，tanh，elu，softplus，softsign，连续但不是处处可微的函数relu，relu6，crelu，relu_x，随机正则化函数dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "tf.nn.*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sigmoid\n",
    "\n",
    "$$S(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "$$S(x) \\in (0, +\\infty) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.7310586   0.88079703]\n",
      " [ 0.7310586   0.88079703]\n",
      " [ 0.7310586   0.88079703]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant([[1.0, 2.0], [1.0, 2.0], [1.0, 2.0]])\n",
    "sess = tf.Session()\n",
    "print(sess.run(tf.nn.sigmoid(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tanh\n",
    "\n",
    "$$ tanh(x) = \\frac{1-e^{-2x}}{1+e^{-2x}} $$\n",
    "$$ tanh(x) \\in (-\\infty, +\\infty) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.76159418  0.96402758]\n",
      " [ 0.76159418  0.96402758]\n",
      " [ 0.76159418  0.96402758]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant([[1.0, 2.0], [1.0, 2.0], [1.0, 2.0]])\n",
    "sess = tf.Session()\n",
    "print(sess.run(tf.nn.tanh(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ReLU\n",
    "\n",
    "$$f(x)=\\max(x,0) $$\n",
    "$$f(x) \\in [0, +\\infty) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.]\n",
      " [ 1.  2.]\n",
      " [ 1.  2.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant([[1.0, 2.0], [1.0, 2.0], [1.0, 2.0]])\n",
    "sess = tf.Session()\n",
    "print(sess.run(tf.nn.relu(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* softplus (ReLU的平滑版本）\n",
    "\n",
    "$$f(x) = log(1+e^x) $$\n",
    "$$f(x) \\in [0, +\\infty) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.31326163  2.12692809]\n",
      " [ 1.31326163  2.12692809]\n",
      " [ 1.31326163  2.12692809]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant([[1.0, 2.0], [1.0, 2.0], [1.0, 2.0]])\n",
    "sess = tf.Session()\n",
    "print(sess.run(tf.nn.softplus(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dropout  以概率keep_prob决定是否被抑制，如果被抑制，该神经元的输出就是0，如果不被抑制，该神经元的输出值将放大到原来的1/keep_prob倍\n",
    ">默认情况下每个神经元是否被抑制是相互独立的；也可以用过noise_shape来调节。若shape(x)=[k,l,m,n]，维度分别为批、行、列、通道，noise_shape=[k,1,1,n]，则批和通道是相互独立的每行和每列的数据是关联的，要么都是0，要么都是原来的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.  0.  6.  8.]]\n",
      "[[-2.  4.  6.  8.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant([[-1.0, 2.0, 3.0, 4.0]])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    b = tf.nn.dropout(a, 0.5, noise_shape= [1, 4])\n",
    "    print(sess.run(b))\n",
    "    b = tf.nn.dropout(a, 0.5, noise_shape= [1, 1])\n",
    "    print(sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**激活函数的选择**\n",
    "* 输入特征相差明显，用tanh；特征相差不明显，用sigmoid\n",
    "* tanh和sigmoid需要规范化，防止落入平坦区\n",
    "* ReLU可以避免规范化，是目前的主流"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.nn.convolution（input, filter, padding, strides = None, dilation_rate = None, name = None, data_format = None)\n",
    " >计算N维卷积的和\n",
    "* tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu = None, data_format = None, name = None)\n",
    " >对一个四维的输入数据'input'和四维的卷积核'filter'，做一个二维卷积。给定一个输入张量[batch, in_height, in_width, in_channels]和一个张量filter [filter_height, filter_width, **in_channels**, **out_channels**]， 进行如下操作：\n",
    "     * 把filter展开成一个二维矩阵： [filter_height \\* filter_width \\* in_channels, output_channels]\n",
    "     * 从输入张量中提取图像patches，形成一个**virtual**的张量[batch, out_height, out_width, filter_height \\* filter_widdth \\* in_channels]\n",
    "     * 对每个patch，右乘filter matrix \\* image patch vector\n",
    ">input: 必须是float32或者float64\n",
    ">filter: 必须和input类型相同\n",
    ">strides: 长度为4的一维list，每一项对应input中每一维度对应的移动步长\n",
    ">padding: “SAME\", \"VALID\"\n",
    ">data_format: \"NHWC\"(default),\"NCHW\"\n",
    "    * NHWC: 数据存储顺序是[batch, in_height, in_width, in_channels]\n",
    "    * NCHW: [batch, in_channels, in_height, in_width]\n",
    ">返回值：和input同样类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "input_data = tf.Variable(np.random.rand(1, 3, 3, 1), dtype = np.float32) #一张3*3的图像（中间两位）\n",
    "filter_data = tf.Variable(np.random.rand(1, 1, 1, 1), dtype = np.float32) #一个1*1的卷积核（前两位）\n",
    "y = tf.nn.conv2d(input_data, filter_data, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "\n",
    "with tf.Session() as sess2:\n",
    "    sess2.run(tf.global_variables_initializer())\n",
    "    print(sess2.run(y))\n",
    "    print(sess2.run(tf.shape(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.nn.depthwise_conv2d(input, filter, strides, padding, rate = None, name = None, data_format = None)\n",
    "* tf.nn.separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, rate = None, name = None, data_format = None)\n",
    "* tf.nn.atrous_conv2d(value, filter, rate, padding, name = None)\n",
    "* tf.nn.conv2d_transpose(value, filter, output_shape, strides, padding = 'SAME', data_format = 'NHWC', name = None)\n",
    ">value是输入图像，filter是卷积核[filter_height, filter_width, **out_channels, in_channels**]，具体含义是[卷积核的高度，卷积核的宽度，卷积核个数，图像通道数]。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://img.blog.csdn.net/20170512135421512\" style=\"zoom:50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.nn.conv1d(value, filters, stride, padding, use_cudnn_on_gpu = None, data_format = None, name = None)\n",
    ">输入是三维[batch, in_width, in_channels]， filter也是三维，stride是一个正整数\n",
    "* tf.nn.conv3d(input, filter, strides, padding, name = None)\n",
    "> 五维输入，输入[batch, in_depth, in_height, in_width, in_channels];filter[filter_depth, filter_height, filter_width, in_channels, out_channels]; strides[strides_batch, strides_width, strides_height, strides_channel]\n",
    "* tf.nn.conv3d_transpose(value, filter, output_shape, strides, padding = 'SAME', name = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 池化函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "池化操作的矩阵窗口大小是由ksize指定的，并且根据步长strides决定移动步长\n",
    "* tf.nn.avg_pool(value,看似则， strides， padding， data_format= ‘NHWC', name = None)\n",
    ">value是四维张量[batch, height, width, channels], ksize长度不小于4 的整形数组，每一位的值对应value每一维上的窗口值，strides长度不小于4的数组，每一位是在value每一维上的移动步长。shape(output)=(shape(value)-ksize+1)/strides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.nn.max_pool\n",
    "* tf.nn.max_pool_with_argmax\n",
    "> 计算最大值的同时计算最大值所在位置，索引位置是((b\\*height+y)\\*width+x)\\*channels+c;返回结果是(output, argmax), argmax的数据类型是Targmax，维度是四维\n",
    "* tf.nn.avg_pool3d/tf.nn.max_pool3d\n",
    "* tf.nn.fractional_avg_pool/tf.nn.fractional_max_pool\n",
    "* tf.nn.pool 通用池化操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name = None)\n",
    "> 输入：logits:[batch_size, num_classes], targets: [batch_size, size].logits用最后一层的输入即可。最后一层不需要进行sigmoid运算，此函数内部进行了sigmoid操作.输出：loss: [batch_size, num_classes]\n",
    "* tf.nn.softmax(logits, dim = -1, name = None)\n",
    "> 计算Softmax激活，softmax = exp(logits) / reduce_sum(exp(logits, dim))\n",
    "* tf.nn.log_softmax(logits, dim = -1, name = None)\n",
    "> 计算log(Softmax)激活， logSoftmax = logits - log (reduce_sum(exp(logits, dim)))\n",
    "* tf.nn.softmax_cross_entropy_with_logits(\\_sentinel = None, labels = None, logits = None, dim = -1, name = None)\n",
    "> 输入 logits和labels均为[batch_size, num_classes];输出loss: [batch_size], 里面是batch中每个样本的交叉熵\n",
    "* tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name = None)\n",
    ">logits是网络最后一层的结果， logits: [batch_size, num_classes], labels: [batch_size];输出 loss[batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.train.GradientDescentOptimizer\n",
    "* tf.train.AdadeltaOptimizer\n",
    "* tf.train.AdagradOptimizer\n",
    "* tf.train.MomentumOptimizer\n",
    "* tf.train.AdamOptimizer\n",
    "* tf.train.FtrlOptimizer\n",
    "* tf.train.RMSPropOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在不怎么调整参数的情况下，Adagrad法比SGD和Momentum法更稳定，性能更优；精调参数的情况下，SGD法和Momentum法在收敛速度和准确性上优于Adagrad法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型的存储与加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 生成检查点文件.ckpt\n",
    "* 生成图协议文件.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型的存储与加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立一个tf.train.Saver()保存变量，并且指定保存位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "mnist = input_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
