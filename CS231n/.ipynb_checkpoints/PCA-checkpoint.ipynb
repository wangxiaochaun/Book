{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内积与投影"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(a_1,a_2,\\cdots,a_n)^\\mathsf{T}\\cdot (b_1,b_2,\\cdots,b_n)^\\mathsf{T}=a_1b_1+a_2b_2+\\cdots+a_nb_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A与B的内积等于A到B的投影长度乘以B的模。再进一步，如果我们假设B的模为1，即让|B|=1，那么就变成了："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A\\cdot B=|A|cos(a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是说，**设向量B的模为1，则A与B的内积值等于A向B所在直线投影的矢量长度！**这就是内积的一种几何解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个向量(3,2)实际是说在x轴投影为3而y轴的投影为2。注意投影是一个矢量，所以可以为负。更正式的说，向量(x,y)实际上表示线性组合："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x(1,0)^\\mathsf{T}+y(0,1)^\\mathsf{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了。只不过我们经常省略第一步，而默认以(1,0)和(0,1)为基。\n",
    "\n",
    "*任何两个线性无关的二维向量都可以成为一组基*   \n",
    "\n",
    "对于基 $(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}})和(-\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}})$，有：\n",
    "\n",
    "现在，我们想获得(3,2)在新基上的坐标，即在两个方向上的投影矢量值，那么根据内积的几何意义，我们只要分别计算(3,2)和两个基的内积，不难得到新的坐标为$(\\frac{5}{\\sqrt{2}},-\\frac{1}{\\sqrt{2}})$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://blog.codinglabs.org/uploads/pictures/pca-tutorial/05.png\"\n",
    "style = \"wdith: 400px; height: 400px; float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基变换的矩阵表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\\\ -1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 5/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中矩阵的两行分别为两个基，乘以原向量，其结果刚好为新基的坐标。可以稍微推广一下，如果我们有m个二维向量，只要将二维向量按列排成一个两行m列矩阵，然后用“基矩阵”乘以这个矩阵，就得到了所有这些向量在新基下的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\\\ -1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix} \\begin{pmatrix} 1 & 2 & 3 \\\\ 1 & 2 & 3 \\end{pmatrix} = \\begin{pmatrix} 2/\\sqrt{2} & 4/\\sqrt{2} & 6/\\sqrt{2} \\\\ 0 & 0 & 0 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**一般的，如果我们有M个N维向量，想将其变换为由R个N维向量表示的新空间中，那么首先将R个基按行组成矩阵A，然后将向量按列组成矩阵B，那么两矩阵的乘积AB就是变换结果，其中AB的第m列为A中第m列变换后的结果。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{pmatrix} p_1 \\\\ p_2 \\\\ \\vdots \\\\ p_R \\end{pmatrix} \\begin{pmatrix} a_1 & a_2 & \\cdots & a_M \\end{pmatrix} = \\begin{pmatrix} p_1a_1 & p_1a_2 & \\cdots & p_1a_M \\\\ p_2a_1 & p_2a_2 & \\cdots & p_2a_M \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ p_Ra_1 & p_Ra_2 & \\cdots & p_Ra_M \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*对每个向量$a_{i}$，实际上是施加了一个线性变换$a_{i}'=Pa_{i}$。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述分析同时给矩阵相乘找到了一种物理解释：两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。更抽象的说，一个矩阵可以表示一种线性变换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 协方差矩阵及优化目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们有一组N维向量，现在要将其降到K维（K小于N），那么我们应该如何选择K个基才能最大程度保留原有的信息？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "五条消息：$\\begin{pmatrix} 1 & 1 & 2 & 4 & 2 \\\\ 1 & 3 & 3 & 4 & 4 \\end{pmatrix}$，归一化为：$\\begin{pmatrix} -1 & -1 & 0 & 2 & 0 \\\\ -2 & 0 & 0 & 1 & 1 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://blog.codinglabs.org/uploads/pictures/pca-tutorial/06.png\"\n",
    "style=\"width:400px;height:400px;float:left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们必须使用一维来表示这些数据，又希望尽量保留原始的信息，你要如何选择？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一种直观的看法是：希望投影后的投影值尽可能分散。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而这种分散程度，可以用数学上的**方差**来表述。$Var(a)=\\frac{1}{m}\\sum_{i=1}^m{(a_i-\\mu)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数学上可以用两个字段的**协方差**表示其相关性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们得到了降维问题的优化目标：**将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终要达到的目的与字段内方差及字段间协方差有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们来了灵感："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们只有a和b两个字段，那么我们将它们按行组成矩阵X：$X=\\begin{pmatrix} a_1 & a_2 & \\cdots & a_m \\\\ b_1 & b_2 & \\cdots & b_m \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们用X乘以X的转置，并乘上系数1/m："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{1}{m}XX^\\mathsf{T}=\\begin{pmatrix} \\frac{1}{m}\\sum_{i=1}^m{a_i^2} & \\frac{1}{m}\\sum_{i=1}^m{a_ib_i} \\\\ \\frac{1}{m}\\sum_{i=1}^m{a_ib_i} & \\frac{1}{m}\\sum_{i=1}^m{b_i^2} \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据矩阵相乘的运算法则，这个结论很容易被推广到一般情况："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设$C=\\frac{1}{m}XX^\\mathsf{T}$，则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**根据上述推导，我们发现要达到优化目前，等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{array}{l l l} D & = & \\frac{1}{m}YY^\\mathsf{T} \\\\ & = & \\frac{1}{m}(PX)(PX)^\\mathsf{T} \\\\ & = & \\frac{1}{m}PXX^\\mathsf{T}P^\\mathsf{T} \\\\ & = & P(\\frac{1}{m}XX^\\mathsf{T})P^\\mathsf{T} \\\\ & = & PCP^\\mathsf{T} \\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在事情很明白了！我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了寻找一个矩阵P，满足$PCP^\\mathsf{T}$是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上文知道，协方差矩阵C是一个是对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质：\n",
    "\n",
    "1）实对称矩阵不同特征值对应的特征向量必然正交。\n",
    "\n",
    "2）设特征向量$\\lambda$重数为r，则必然存在r个线性无关的特征向量对应于$\\lambda$，因此可以将这r个特征向量单位正交化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为$e_1,e_2,\\cdots,e_n$，我们将其按列组成矩阵："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E=\\begin{pmatrix} e_1 & e_2 & \\cdots & e_n \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则对协方差矩阵C有如下结论："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E^\\mathsf{T}CE=\\Lambda=\\begin{pmatrix} \\lambda_1 & & & \\\\ & \\lambda_2 & & \\\\ & & \\ddots & \\\\ & & & \\lambda_n \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这里，我们发现我们已经找到了需要的矩阵P：\n",
    "\n",
    "$P=E^{\\mathsf{T}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照$\\Lambda$中特征值的从大到小，将特征向量从上到下排列，则用P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{pmatrix} -1 & -1 & 0 & 2 & 0 \\\\ -2 & 0 & 0 & 1 & 1 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用PCA方法将其降维到一维。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先归一化（这个词用的不准确，具体是说使得每个字段（特征）的均值为0），然后求协方差矩阵："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C=\\frac{1}{5}\\begin{pmatrix} -1 & -1 & 0 & 2 & 0 \\\\ -2 & 0 & 0 & 1 & 1 \\end{pmatrix}\\begin{pmatrix} -1 & -2 \\\\ -1 & 0 \\\\ 0 & 0 \\\\ 2 & 1 \\\\ 0 & 1 \\end{pmatrix}=\\begin{pmatrix} \\frac{6}{5} & \\frac{4}{5} \\\\ \\frac{4}{5} & \\frac{6}{5} \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后求其特征值和特征向量："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda_1=2,\\lambda_2=2/5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其对应的特征向量分别是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$c_1\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix},c_2\\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中对应的特征向量分别是一个通解，$c_1,c_2$可以取任意实数。标准化后的特征向量为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{pmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{pmatrix},\\begin{pmatrix} -1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵P是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P=\\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\\\ -1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*就是把特征向量按行的形式写了*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证对角化："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$PCP^\\mathsf{T}=\\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\\\ -1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}\\begin{pmatrix} 6/5 & 4/5 \\\\ 4/5 & 6/5 \\end{pmatrix}\\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\\\ 1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}=\\begin{pmatrix} 2 & 0 \\\\ 0 & 2/5 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "降维的话，去特征值最大的，也就是$\\lambda_1$，对应特征向量转置（行表示）乘以数据矩阵，就得到了降维后的表示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y=\\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}\\begin{pmatrix} -1 & -1 & 0 & 2 & 0 \\\\ -2 & 0 & 0 & 1 & 1 \\end{pmatrix}=\\begin{pmatrix} -3/\\sqrt{2} & -1/\\sqrt{2} & 0 & 3/\\sqrt{2} & -1/\\sqrt{2} \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "降维（投影）结果如下图："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://blog.codinglabs.org/uploads/pictures/pca-tutorial/07.png\"\n",
    "style=\"weight:400px;height:400px;float:left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讨论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后需要说明的是，PCA是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以PCA便于通用实现，但是本身无法个性化的优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "114px",
    "width": "256px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
